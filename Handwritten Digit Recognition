import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np

print(f"TensorFlow Version: {tf.__version__}")

# --- 1. Load the MNIST Dataset ---
# Keras provides the MNIST dataset built-in.
# x_train/x_test are the images (the "problem")
# y_train/y_test are the labels (the "answer", e.g., "5", "7")
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

print(f"Loaded {len(x_train)} images for training.")
print(f"Loaded {len(x_test)} images for testing.")
print(f"Image shape: {x_train[0].shape}") # Images are 28x28 pixels

# --- 2. Preprocess the Data ---
# Models train best when pixel values are normalized from 0.0 to 1.0,
# instead of 0 to 255.
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Add a "channel" dimension. Grayscale images have 1 channel.
# (60000, 28, 28) -> (60000, 28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

print(f"New image shape: {x_train[0].shape}")

# --- 3. Build the AI Model (The "Identification Modal") ---
# We will build a Convolutional Neural Network (CNN),
# which is the standard for image recognition.

model = keras.Sequential([
    # Input layer. Specify the shape of one image.
    keras.Input(shape=(28, 28, 1)),
    
    # Convolutional Layer: Learns 32 different "filters" (features)
    layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
    
    # Pooling Layer: Shrinks the image to focus on important features
    layers.MaxPooling2D(pool_size=(2, 2)),
    
    # Another Conv/Pool pair to learn more complex features
    layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2, 2)),
    
    # Flatten: Turns the 2D image data into a 1D list
    layers.Flatten(),
    
    # A "Dense" (standard) layer with 128 neurons
    layers.Dense(128, activation="relu"),
    
    # Output Layer: 10 neurons, one for each digit (0-9).
    # "softmax" gives a probability for each digit.
    layers.Dense(10, activation="softmax")
])

# Show a summary of the model we just built
model.summary()

# --- 4. Compile the Model ---
# Tell the model how to learn
model.compile(
    optimizer="adam",  # A good, all-purpose optimizer
    loss="sparse_categorical_crossentropy", # Best for "digit 0", "digit 1", etc.
    metrics=["accuracy"] # We want to see how accurate it is
)

# --- 5. Train the Model ---
print("\n--- Starting Training ---")
# This is where the model "learns" by looking at all the images.
# An "epoch" is one full pass over the entire training dataset.
history = model.fit(
    x_train,
    y_train,
    batch_size=64,
    epochs=5, # 5 epochs is a good start
    validation_data=(x_test, y_test) # Test against unseen data
)
print("--- Training Finished ---")

# --- 6. Evaluate the Model ---
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"\nTest Accuracy: {test_accuracy * 100:.2f}%")

# --- 7. Make a Prediction ---
print("\n--- Making a Prediction ---")
# Let's take the first image from the test set
image_to_test = x_test[0]
label_of_image = y_test[0]

# Add a "batch" dimension (models expect batches of images)
# (28, 28, 1) -> (1, 28, 28, 1)
image_for_prediction = np.expand_dims(image_to_test, 0)

# Get the model's prediction
predictions = model.predict(image_for_prediction)

# The prediction is an array of 10 probabilities
print(f"Model's raw predictions: {predictions}")

# Find the digit with the highest probability
predicted_digit = np.argmax(predictions[0])

print(f"\nModel predicted: {predicted_digit}")
print(f"Actual label was: {label_of_image}")

# Show the image so you can see it for yourself
plt.figure()
plt.imshow(np.squeeze(image_to_test), cmap="gray")
plt.title(f"Predicted: {predicted_digit}, Actual: {label_of_image}")
plt.show()
